# Fun√ß√µes para carregar dados (schema, metadados, contagens) 

import streamlit as st # Necess√°rio para decorators de cache e st.session_state
import os
import json
import logging
from collections import OrderedDict
import time
import copy
import faiss # Importar faiss aqui
from functools import lru_cache

# Importar de outros m√≥dulos core
import src.core.config as config
from src.utils.json_helpers import load_json, save_json # Mant√©m
from src.analysis.analysis import analyze_key_structure # <-- ADICIONADO # Atualizado

logger = logging.getLogger(__name__)

# --- Fun√ß√µes de Carregamento de Arquivos --- #

@st.cache_data # Cache para estrutura t√©cnica (n√£o muda na sess√£o)
def load_technical_schema(file_path):
    logger.info(f"---> EXECUTANDO load_technical_schema para: {file_path}") # Log de diagn√≥stico
    """Carrega o schema t√©cnico (combinado) do arquivo JSON."""
    if not os.path.exists(file_path):
        st.error(f"Erro: Arquivo de schema t√©cnico n√£o encontrado em '{file_path}'")
        logger.error(f"Erro: Arquivo de schema t√©cnico n√£o encontrado em '{file_path}'")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f, object_pairs_hook=OrderedDict)
            logger.info(f"Schema t√©cnico carregado de {file_path}")
            return data
    except json.JSONDecodeError as e:
        st.error(f"Erro ao decodificar JSON do schema t√©cnico {file_path}: {e}")
        logger.error(f"Erro ao decodificar JSON do schema t√©cnico {file_path}: {e}", exc_info=True)
        return None
    except Exception as e:
        st.error(f"Erro inesperado ao carregar schema t√©cnico {file_path}: {e}")
        logger.error(f"Erro inesperado ao carregar schema t√©cnico {file_path}: {e}", exc_info=True)
        return None

# @st.cache_data # <<< TEMPORARIAMENTE REMOVIDO PARA DEBUG
def load_metadata(file_path):
    logger.info("---> EXECUTANDO load_metadata (SEM CACHE) para: {file_path}") # Log de diagn√≥stico
    """Carrega o arquivo JSON de metadados."""
    if not os.path.exists(file_path):
        # Log warning em vez de error, pois pode ser iniciado vazio
        logger.warning(f"Arquivo de metadados {file_path} n√£o encontrado ou ainda n√£o criado. Retornando dict vazio.")
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # Usar OrderedDict para tentar manter a ordem original das chaves
            data = json.load(f, object_pairs_hook=OrderedDict)
            logger.info(f"Metadados carregados de {file_path}")
            # --- NOVO: Normalizar valores None para "" --- #
            _normalize_none_to_empty_string(data)
            logger.debug("Metadados normalizados (None -> \"\").")
            # --- FIM NOVO --- #
            return data
    except json.JSONDecodeError as e:
        st.error(f"Erro ao decodificar JSON do arquivo {file_path}: {e}")
        logger.error(f"Erro ao decodificar JSON do arquivo {file_path}: {e}", exc_info=True)
        return {}
    except IOError as e:
        st.error(f"Erro ao ler o arquivo {file_path}: {e}")
        logger.error(f"Erro ao ler o arquivo {file_path}: {e}", exc_info=True)
        return {}
    except Exception as e:
        st.error(f"Erro inesperado ao carregar {file_path}: {e}")
        logger.exception(f"Erro inesperado ao carregar {file_path}")
        return {}

# --- NOVA FUN√á√ÉO AUXILIAR DE NORMALIZA√á√ÉO ---
FIELDS_TO_NORMALIZE = {
    'description', 
    'object_business_description', 
    'object_value_mapping_notes', 
    'business_description', 
    'value_mapping_notes'
}

def _normalize_none_to_empty_string(obj):
    """Percorre recursivamente dicts e lists, substituindo None por "" em chaves espec√≠ficas."""
    if isinstance(obj, dict):
        for key, value in obj.items():
            if key in FIELDS_TO_NORMALIZE and value is None:
                obj[key] = ""
            elif isinstance(value, (dict, list)):
                _normalize_none_to_empty_string(value)
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            if isinstance(item, (dict, list)):
                _normalize_none_to_empty_string(item)
# --- FIM DA NOVA FUN√á√ÉO ---

@st.cache_data # Cache para contagens (n√£o devem mudar frequentemente sem a√ß√£o externa)
def load_overview_counts(file_path):
    """Carrega as contagens e timestamps da vis√£o geral."""
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Aviso: Arquivo de contagens '{file_path}' inv√°lido.")
            return {}
        except Exception as e:
            logger.error(f"Erro inesperado ao carregar contagens: {e}", exc_info=True)
            return {}
    else:
        logger.info(f"Arquivo de contagens '{file_path}' n√£o encontrado. Contagens n√£o ser√£o exibidas.")
        return {}

# --- NOVA Fun√ß√£o Cacheada para Carregar FAISS --- #
@st.cache_resource(ttl=3600) # Cache do √≠ndice por 1 hora (ou ajuste conforme necess√°rio)
def load_faiss_index(index_path):
    logger.info(f"---> EXECUTANDO load_faiss_index para: {index_path}")
    """Carrega o √≠ndice FAISS do arquivo especificado."""
    if not os.path.exists(index_path):
        logger.warning(f"Arquivo de √≠ndice FAISS n√£o encontrado em '{index_path}'. Retornando None.")
        return None
    try:
        start_time = time.time()
        index = faiss.read_index(index_path)
        end_time = time.time()
        logger.info(f"√çndice FAISS carregado de {index_path} em {end_time - start_time:.2f}s. Vetores: {index.ntotal}")
        return index
    except Exception as e:
        st.error(f"Erro ao carregar √≠ndice FAISS de '{index_path}': {e}")
        logger.error(f"Erro ao carregar √≠ndice FAISS de '{index_path}': {e}", exc_info=True)
        return None

# --- NOVA Fun√ß√£o Cacheada para Carregar An√°lise de Chaves --- #
@st.cache_data
def load_key_analysis_results(file_path):
    logger.info(f"---> EXECUTANDO load_key_analysis_results para: {file_path}")
    """Carrega os resultados pr√©-calculados da an√°lise de chaves."""
    # Retorna um dicion√°rio com chaves esperadas mesmo em caso de falha,
    # para evitar KeyErrors posteriores.
    default_result = {
        "composite_pk_tables": {},
        "junction_tables": {},
        "composite_fk_details": {},
        "column_roles": {}
    }
    
    # --- NOVO: Verificar se file_path √© None --- #
    if file_path is None:
        logger.warning("Nenhum arquivo de an√°lise de chaves fornecido. Usando resultados vazios.")
        return default_result
    # --- FIM NOVO ---
    
    if not os.path.exists(file_path):
        logger.warning(f"Arquivo de resultados da an√°lise de chaves n√£o encontrado em '{file_path}'. Usando resultados vazios.")
        return default_result
    try:
        start_time = time.time()
        # Usa a fun√ß√£o load_json importada que j√° tem tratamento de erro
        data = load_json(file_path)
        end_time = time.time()
        if data is None:
             logger.error(f"Falha ao carregar ou decodificar JSON de '{file_path}'. Usando resultados vazios.")
             return default_result
        else:
            # Verifica se as chaves esperadas existem (opcional, mas bom)
            missing_keys = set(default_result.keys()) - set(data.keys())
            if missing_keys:
                 logger.warning(f"Arquivo {file_path} carregado, mas faltam chaves: {missing_keys}. Usando defaults para chaves ausentes.")
                 # Preenche chaves ausentes com default
                 for key in missing_keys:
                     data[key] = default_result[key]
                     
            logger.info(f"Resultados da an√°lise de chaves carregados de {file_path} em {end_time - start_time:.2f}s.")
            return data
    except Exception as e:
        st.error(f"Erro inesperado ao carregar resultados da an√°lise de chaves de '{file_path}': {e}")
        logger.error(f"Erro inesperado ao carregar resultados da an√°lise de chaves de '{file_path}': {e}", exc_info=True)
        return default_result

# --- NOVO: Fun√ß√£o para carregar descri√ß√µes AI de objetos --- #
@st.cache_data
def load_ai_object_descriptions(file_path):
    """Carrega o arquivo JSON com descri√ß√µes AI para objetos."""
    logger.info(f"---> EXECUTANDO load_ai_object_descriptions para: {file_path}")
    if not os.path.exists(file_path):
        logger.warning(f"Arquivo de descri√ß√µes AI de objetos {file_path} n√£o encontrado. Retornando dict vazio.")
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f, object_pairs_hook=OrderedDict)
            logger.info(f"Descri√ß√µes AI de objetos carregadas de {file_path}")
            return data
    except Exception as e:
        st.error(f"Erro ao carregar/decodificar JSON de descri√ß√µes AI de objetos {file_path}: {e}")
        logger.error(f"Erro ao carregar descri√ß√µes AI de objetos {file_path}", exc_info=True)
        return {}

# --- Fun√ß√£o Principal de Carregamento e Processamento --- #

def load_and_process_data():
    # --- Cache foi removido de load_metadata, clear() n√£o √© mais necess√°rio/v√°lido --- #
    # load_metadata.clear()
    # logger.info("Cache de load_metadata limpo no in√≠cio de load_and_process_data.")
    # ---------------------------------------------------------------------------- #
    # --- Configura√ß√£o da Barra de Progresso e Tempos --- #
    total_steps = 7 # Incrementa o total de passos
    progress_bar = st.progress(0.0, text="Iniciando carregamento...")
    start_time_total = time.time()
    step_times = {}
    current_step = 0

    def update_progress(step_name, step_start_time):
        nonlocal current_step
        duration = time.time() - step_start_time
        step_times[step_name] = duration
        current_step += 1
        progress_value = float(current_step) / total_steps
        try:
            progress_bar.progress(progress_value, text=f"({current_step}/{total_steps}) {step_name} conclu√≠da em {duration:.2f}s...")
        except st.runtime.scriptrunner.ScriptStoppException:
             logger.warning(f"Script parado durante {step_name}")
             raise # Re-raise para parar a execu√ß√£o
        except Exception as e:
             logger.error(f"Erro ao atualizar barra de progresso em {step_name}: {e}")
        logger.info(f"Etapa '{step_name}' conclu√≠da em {duration:.2f}s")

    # --- Determinar qual schema carregar inicialmente --- #
    use_embeddings_on_load = st.session_state.get('use_embeddings', False)
    if use_embeddings_on_load:
        # Se usar embeddings, carrega o arquivo que j√° cont√©m os embeddings
        schema_file_to_load = config.EMBEDDED_SCHEMA_FILE
        faiss_index_file_to_load = config.FAISS_INDEX_FILE
        analysis_file_to_load = config.KEY_ANALYSIS_RESULTS_FILE # Assumindo que a an√°lise foi salva com base no schema com embeddings
        logger.info(f"Carregamento inicial: Usando schema com embeddings ({schema_file_to_load}), √≠ndice FAISS ({faiss_index_file_to_load}) e an√°lise pr√©-calculada ({analysis_file_to_load}).")
    else:
        # Se N√ÉO usar embeddings, carrega o schema mesclado mais recente (t√©cnico + manual + AI)
        schema_file_to_load = config.MERGED_SCHEMA_FOR_EMBEDDINGS_FILE 
        faiss_index_file_to_load = None # N√£o carrega FAISS se n√£o usar embeddings
        analysis_file_to_load = None # N√£o carrega an√°lise se n√£o usar embeddings
        logger.info(f"Carregamento inicial: Usando schema mesclado mais recente ({schema_file_to_load}).")

    # --- Etapa 1: Carregar Schema (Mesclado ou com Embeddings) --- #
    step_name = "Carregando Schema"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
    schema_loaded = load_technical_schema(schema_file_to_load)
    if schema_loaded is None:
        st.error(f"Falha cr√≠tica: N√£o foi poss√≠vel carregar o arquivo de schema em '{schema_file_to_load}'.")
        st.stop()
    # --- NOVO: Armazenar path do schema carregado ---
    st.session_state.loaded_schema_file = schema_file_to_load
    # --- FIM NOVO ---
    update_progress(step_name, start_time_step)

    # --- Etapa 2: Carregar Metadados --- #
    step_name = "Carregando Metadados"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
    metadata_dict = load_metadata(config.METADATA_FILE)
    
    # Se o JSON de metadados n√£o estiver aninhado sob 'TABLES' ou 'VIEWS', encapsula em 'TABLES'
    if not isinstance(metadata_dict, dict):
        metadata_dict = {}
    if 'TABLES' not in metadata_dict and 'VIEWS' not in metadata_dict:
        metadata_dict = {'TABLES': metadata_dict}
    
    # Normalizar chave 'columns' de cada objeto para 'COLUMNS' (upper case) para compatibilidade com UI
    for obj_type in ['TABLES', 'VIEWS']:
        if obj_type in metadata_dict and isinstance(metadata_dict[obj_type], dict):
            for obj_name, obj_meta in metadata_dict[obj_type].items():
                if isinstance(obj_meta, dict) and 'columns' in obj_meta:
                    # Evita sobrescrever COLUMNS se j√° existir
                    if 'COLUMNS' not in obj_meta:
                        obj_meta['COLUMNS'] = obj_meta.pop('columns')
                    else:
                        # Se j√° existir COLUMNS, apenas remova o antigo
                        obj_meta.pop('columns', None)
    
    # --- ADICIONADO: Verifica√ß√£o Estrita --- #
    if not metadata_dict or not isinstance(metadata_dict, dict):
        logger.error("Falha cr√≠tica ao carregar metadados ou metadados vazios. Interrompendo.")
        st.error(f"Erro Cr√≠tico: N√£o foi poss√≠vel carregar dados v√°lidos de {config.METADATA_FILE}. Verifique o arquivo e os logs.")
        st.stop()
    # --- FIM Verifica√ß√£o Estrita --- #
        
    logger.info(f"Metadados carregados com sucesso. Chaves de n√≠vel superior: {list(metadata_dict.keys())}")
    update_progress(step_name, start_time_step)

    # --- Etapa 3: Carregar Contagens da Vis√£o Geral --- #
    step_name = "Carregando Contagens (Cache)"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
    overview_counts = load_overview_counts(config.OVERVIEW_COUNTS_FILE)
    update_progress(step_name, start_time_step)

    # --- Etapa X (NOVA): Carregar Descri√ß√µes AI de Objetos --- #
    step_name = "Carregando Descri√ß√µes AI (Objetos)"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
    ai_object_descriptions = load_ai_object_descriptions(config.AI_OBJECT_DESCRIPTIONS_FILE)
    update_progress(step_name, start_time_step)

    # --- Etapa 4: CARREGAR √çndice FAISS (se aplic√°vel) --- #
    step_name = "Carregando √çndice FAISS"
    start_time_step = time.time()
    faiss_index = None # Inicializa como None
    if faiss_index_file_to_load:
        progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
        faiss_index = load_faiss_index(faiss_index_file_to_load)
        # Se o carregamento falhar, faiss_index ser√° None
    else:
        logger.info("√çndice FAISS n√£o ser√° carregado (embeddings desativados).")
    update_progress(step_name, start_time_step)
    # TODO: O `index_to_key_map` precisa ser gerado/carregado separadamente ou junto com o √≠ndice FAISS.
    # Assumindo por agora que ele pode ser derivado do `schema_loaded` se embeddings estiverem presentes.
    index_to_key_map = None # Precisa implementar a l√≥gica para gerar/carregar isso

    # --- Etapa 6: Carregar An√°lise de Chaves --- #
    step_name = "Carregando An√°lise de Chaves"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")

    # Tenta carregar os resultados do arquivo JSON gerado pelo script externo
    # load_key_analysis_results retorna um dict (ou um dict padr√£o vazio em caso de erro/n√£o encontrado)
    st.session_state.key_analysis = load_key_analysis_results(config.KEY_ANALYSIS_RESULTS_FILE)
    logger.info(f"Resultados da an√°lise de chaves carregados de {config.KEY_ANALYSIS_RESULTS_FILE} (ou padr√£o vazio se falhou).")

    update_progress(step_name, start_time_step)

    # --- Etapa 7: Inicializar Estado da Sess√£o --- #
    step_name = "Inicializando Estado da Sess√£o"
    start_time_step = time.time()
    progress_bar.progress(float(current_step)/total_steps, text=f"({current_step+1}/{total_steps}) Executando: {step_name}...")
    
    # --- MODIFICADO: Atualizar sempre o estado principal, inicializar o 'initial' condicionalmente --- #
    # Sempre atualiza o estado principal com os dados carregados do arquivo/cache
    st.session_state.metadata = metadata_dict
    st.session_state.technical_schema = schema_loaded
    st.session_state.overview_counts = overview_counts
    st.session_state.faiss_index = faiss_index
    st.session_state.index_to_key_map = index_to_key_map
    st.session_state.ai_object_descriptions = ai_object_descriptions # Salva no estado
    logger.debug("Estados principais (metadata, technical_schema, etc.) atualizados na sess√£o.")

    # Guarda o estado inicial para compara√ß√£o (APENAS se n√£o existir ou se o save/reload resetou)
    if 'initial_metadata' not in st.session_state:
        try:
            # Usar o estado atual da sess√£o, que acabou de ser atualizado
            st.session_state.initial_metadata = copy.deepcopy(st.session_state.metadata)
            logger.info("Estado 'initial_metadata' inicializado/atualizado na sess√£o.")
        except Exception as e:
            logger.error(f"Erro ao fazer deepcopy para initial_metadata: {e}")
            st.session_state.initial_metadata = {}
    else:
        logger.debug("Estado 'initial_metadata' j√° existe na sess√£o (preservado desde o √∫ltimo save/reload).")
        
    # Inicializa outras vari√°veis de estado se necess√°rio (mant√©m a l√≥gica anterior)
    if 'unsaved_changes' not in st.session_state:
        st.session_state.unsaved_changes = False
    if 'current_view' not in st.session_state:
        st.session_state.current_view = 'overview'
    if 'selected_object' not in st.session_state:
        st.session_state.selected_object = None
    if 'selected_column_index' not in st.session_state:
        st.session_state.selected_column_index = None
    if 'selected_object_type' not in st.session_state:
        st.session_state.selected_object_type = None
    if 'ollama_enabled' not in st.session_state:
        st.session_state.ollama_enabled = False
    if 'db_path' not in st.session_state:
        st.session_state.db_path = config.DEFAULT_DB_PATH
    if 'db_user' not in st.session_state:
        st.session_state.db_user = config.DEFAULT_DB_USER
    if 'db_password' not in st.session_state:
        try:
            db_password_loaded = st.secrets.get("database", {}).get("password")
            if not db_password_loaded:
                db_password_loaded = os.getenv("FIREBIRD_PASSWORD")
                if db_password_loaded:
                    logger.info("Senha do DB carregada da vari√°vel de ambiente FIREBIRD_PASSWORD.")
                else:
                    logger.warning("Senha do DB n√£o encontrada em st.secrets nem na env var FIREBIRD_PASSWORD.")
                    db_password_loaded = "" # Evita None
            else:
                logger.info("Senha do DB carregada de st.secrets.")
            st.session_state.db_password = db_password_loaded
        except Exception as e:
            logger.error(f"Erro ao tentar carregar senha do DB de st.secrets/env var: {e}")
            st.session_state.db_password = ""
    if 'db_charset' not in st.session_state:
        st.session_state.db_charset = config.DEFAULT_DB_CHARSET
    if 'latest_db_timestamp' not in st.session_state:
        st.session_state.latest_db_timestamp = None
    if 'last_save_time' not in st.session_state:
        st.session_state.last_save_time = 0
    if 'auto_save_enabled' not in st.session_state:
        st.session_state.auto_save_enabled = False # Ou True se quiser default
    # --- FIM MODIFICADO --- #
    
    # --- Adicionado: Garantir que as credenciais DB estejam no estado --- #
    if 'db_path' not in st.session_state:
        st.session_state.db_path = config.DEFAULT_DB_PATH
    if 'db_user' not in st.session_state:
        st.session_state.db_user = config.DEFAULT_DB_USER
    if 'db_password' not in st.session_state:
        try:
            db_password_loaded = st.secrets.get("database", {}).get("password")
            if not db_password_loaded:
                db_password_loaded = os.getenv("FIREBIRD_PASSWORD")
                if db_password_loaded:
                    logger.info("Senha do DB carregada da vari√°vel de ambiente FIREBIRD_PASSWORD.")
                else:
                    logger.warning("Senha do DB n√£o encontrada em st.secrets nem na env var FIREBIRD_PASSWORD.")
                    db_password_loaded = "" # Evita None
            else:
                logger.info("Senha do DB carregada de st.secrets.")
            st.session_state.db_password = db_password_loaded
        except Exception as e:
            logger.error(f"Erro ao tentar carregar senha do DB de st.secrets/env var: {e}")
            st.session_state.db_password = ""
    if 'db_charset' not in st.session_state:
        st.session_state.db_charset = config.DEFAULT_DB_CHARSET
    # --- Fim Credenciais DB --- #

    update_progress(step_name, start_time_step)

    # --- Finaliza√ß√£o --- #
    total_time = time.time() - start_time_total
    progress_bar.empty() # Limpa a barra de progresso
    st.toast(f"Carregamento inicial conclu√≠do em {total_time:.2f}s!", icon="üéâ")
    logger.info(f"Carregamento inicial conclu√≠do em {total_time:.2f}s.")
    with st.expander("Detalhes do Tempo de Carregamento Inicial", expanded=False):
        for name, duration in step_times.items():
            st.write(f"- {name}: {duration:.3f}s")
        st.write(f"**- Tempo Total:** {total_time:.3f}s") 